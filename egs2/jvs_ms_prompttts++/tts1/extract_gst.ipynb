{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/users/snegishi/M2/espnet/tools/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional, Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from espnet2.tts.gst.style_encoder import StyleEncoder\n",
    "from espnet2.tts.abs_tts import AbsTTS\n",
    "from espnet2.tts.feats_extract.abs_feats_extract import AbsFeatsExtract\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import os\n",
    "from espnet2.fileio.sound_scp import SoundScpReader\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    \"\"\"Construct the parser.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=__doc__,\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"in_folder\", type=Path, help=\"Path to the input kaldi data directory.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"out_folder\",\n",
    "        type=Path,\n",
    "        help=\"Output folder to save the style embedding.\",\n",
    "    )\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class GST(AbsTTS):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # idim: int,\n",
    "        odim: int,\n",
    "        feats_extract: Optional[AbsFeatsExtract],\n",
    "        \n",
    "        adim: int = 384,\n",
    "        gst_tokens: int = 10,\n",
    "        gst_heads: int = 4,\n",
    "        gst_conv_layers: int = 6,\n",
    "        gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128),\n",
    "        gst_conv_kernel_size: int = 3,\n",
    "        gst_conv_stride: int = 2,\n",
    "        gst_gru_layers: int = 1,\n",
    "        gst_gru_units: int = 128,\n",
    "    ):\n",
    "        \"\"\"GST model.\n",
    "        \n",
    "        Reference Code:\n",
    "            /mnt/data/users/snegishi/M2/Satoru-Negishi/espnet/espnet2/tts/fastspeech2/fastspeech2.py  ,321~\n",
    "        \"\"\"\n",
    "        self.gst = StyleEncoder(\n",
    "            idim=odim,  # the input is mel-spectrogram\n",
    "            gst_tokens=gst_tokens,\n",
    "            gst_token_dim=adim,\n",
    "            gst_heads=gst_heads,\n",
    "            conv_layers=gst_conv_layers,\n",
    "            conv_chans_list=gst_conv_chans_list,\n",
    "            conv_kernel_size=gst_conv_kernel_size,\n",
    "            conv_stride=gst_conv_stride,\n",
    "            gru_layers=gst_gru_layers,\n",
    "            gru_units=gst_gru_units,\n",
    "        )\n",
    "        self.feats_extract = feats_extract\n",
    "\n",
    "    def extract_feats(\n",
    "        self,\n",
    "        speech: torch.Tensor,\n",
    "        speech_lengths: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"Extract features.\n",
    "\n",
    "        Args:\n",
    "            speech (Tensor): Input speech feature (T, D).\n",
    "            speech_lengths (Tensor): The length of input speech feature (N,).\n",
    "\n",
    "        Reference Code:\n",
    "            /mnt/data/users/snegishi/M2/Satoru-Negishi/espnet/espnet2/tts/espnet_model.py  ,153~\n",
    "        \"\"\"\n",
    "        if self.feats_extract is not None:\n",
    "            feats, feats_lengths = self.feats_extract(speech, speech_lengths)\n",
    "        else:\n",
    "            # Use precalculated feats (feats_type != raw case)\n",
    "            feats, feats_lengths = speech, speech_lengths\n",
    "        feats_dict = dict(feats=feats, feats_lengths=feats_lengths)\n",
    "\n",
    "        return feats_dict\n",
    "\n",
    "    def __call__(self, speech, speech_lengths):\n",
    "        embeds = self.extract_feats(speech, speech_lengths)\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    \"\"\"Load the model, generate kernel and bandpass plots.\"\"\"\n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args(argv)\n",
    "\n",
    "    # if torch.cuda.is_available() and (\"cuda\" in args.device):\n",
    "    #     device = args.device\n",
    "    # else:\n",
    "    #     device = \"cpu\"\n",
    "\n",
    "\n",
    "    # Prepare spk2utt for mean x-vector\n",
    "    spk2utt = dict()\n",
    "    with open(os.path.join(args.in_folder, \"spk2utt\"), \"r\") as reader:\n",
    "        for line in reader:\n",
    "            details = line.split()\n",
    "            spk2utt[details[0]] = details[1:]\n",
    "\n",
    "    wav_scp = SoundScpReader(os.path.join(args.in_folder, \"wav.scp\"), np.float32)\n",
    "    os.makedirs(args.out_folder, exist_ok=True)\n",
    "    # writer_utt = kaldiio.WriteHelper(\n",
    "    #     \"ark,scp:{0}/xvector.ark,{0}/xvector.scp\".format(args.out_folder)\n",
    "    # )\n",
    "    # writer_spk = kaldiio.WriteHelper(\n",
    "    #     \"ark,scp:{0}/spk_xvector.ark,{0}/spk_xvector.scp\".format(args.out_folder)\n",
    "    # )\n",
    "    writer_spk = {}\n",
    "    gst_encoder = GST(odim=40,feats_extract=None)\n",
    "\n",
    "    for speaker in tqdm(spk2utt):\n",
    "        style_embeds = list()\n",
    "        for utt in spk2utt[speaker]:\n",
    "            in_sr, wav = wav_scp[utt]\n",
    "            # Style Embedding\n",
    "            embeds = gst_encoder(wav, in_sr)\n",
    "            # writer_utt[utt] = np.squeeze(embeds)\n",
    "            style_embeds.append(embeds)\n",
    "\n",
    "        # Speaker Normalization\n",
    "        embeds = np.mean(np.stack(style_embeds, 0), 0)\n",
    "        writer_spk[speaker] = embeds\n",
    "    # writer_utt.close()\n",
    "    # writer_spk.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
